# -*- coding: utf-8 -*-
"""
Created on Fri Dec 14 15:05:23 2018

@author: mk
"""

def cost_function(x, y, w, b):
    N = len(x)
    #print("The given length is: ",N)
    total_error = 0.0
    for i in range(N):
        temp= (y[i] - (w*x[i]+b))**2 
        total_error+=temp/2
        #print("The current value of error is: ", temp)
    return total_error / N
  
def update_weights(w, b, X, Y, learning_rate):
    m_deriv = 0
    b_deriv = 0
    N = len(X)
    for i in range(N):
        # Calculate partial derivatives
        # -x(y - (wx + b))
        m_deriv += -X[i] * (Y[i] - (w*X[i] + b))

        # -(y - (wx + b))
        b_deriv += -(Y[i] - (w*X[i] + b))

    # We subtract because the derivatives point in direction of steepest ascent
    w -= (m_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate

    return w, b
  
n_weight = 1 #some random weight
n_bias = 0 #some random bias
for epoch in range(350):
    #print("Iteration: ", epoch)
    error = cost_function([1,2,3,4,5],[33.8, 35.6, 37.4, 39.2, 41.0],n_weight,n_bias)
    print("Error: ",error)
    n_weight,n_bias = update_weights(n_weight,n_bias,[1,2,3,4,5],[33.8, 35.6, 37.4, 39.2, 41.0],0.06)
    #print("Weight:", n_weight)
    #print("Bias:", n_bias)
